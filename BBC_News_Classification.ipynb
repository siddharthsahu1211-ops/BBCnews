{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c5da820",
   "metadata": {},
   "source": [
    "# BBC News Classification with Logistic Regression\n",
    "\n",
    "This notebook demonstrates how to train a Logistic Regression model to classify BBC news articles into different categories (business, entertainment, politics, sport, tech)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb0de44",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d457b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f962f3",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5e8012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "texts = []\n",
    "categories = []\n",
    "\n",
    "base_path = \"bbc\"\n",
    "\n",
    "for category in os.listdir(base_path):\n",
    "    category_path = os.path.join(base_path, category)\n",
    "    if not os.path.isdir(category_path):\n",
    "        continue\n",
    "    for filename in os.listdir(category_path):\n",
    "        file_path = os.path.join(category_path, filename)\n",
    "        with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
    "            texts.append(f.read())\n",
    "            categories.append(category)\n",
    "\n",
    "bbc_text = pd.DataFrame({\n",
    "    \"text\": texts,\n",
    "    \"category\": categories\n",
    "})\n",
    "\n",
    "print(f\"✓ Loaded {len(bbc_text)} articles\")\n",
    "print(f\"Categories distribution:\\n{bbc_text['category'].value_counts()}\")\n",
    "print(f\"\\nDataset shape: {bbc_text['category'].shape}\")\n",
    "bbc_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425bafee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame as CSV for future use\n",
    "csv_path = \"bbc-text.csv\"\n",
    "bbc_text.to_csv(csv_path, index=False)\n",
    "print(f\"✓ CSV file saved to: {csv_path}\")\n",
    "print(f\"CSV file size: {os.path.getsize(csv_path) / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a835365c",
   "metadata": {},
   "source": [
    "## Load CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd49556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read our dataset using read_csv()\n",
    "bbc_text = pd.read_csv('bbc-text.csv')\n",
    "\n",
    "# Rename 'text' column to 'News_Headline' for clarity\n",
    "bbc_text = bbc_text.rename(columns={'text': 'News_Headline'}, inplace=False)\n",
    "\n",
    "print(f\"✓ CSV file loaded successfully\")\n",
    "print(f\"Dataset shape: {bbc_text.shape}\")\n",
    "print(f\"Columns: {bbc_text.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "bbc_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97f5e98",
   "metadata": {},
   "source": [
    "## Data Visualization - Articles per Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a55039e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplt\u001b[49m.figure(figsize=(\u001b[32m8\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m      2\u001b[39m sns.countplot(\n\u001b[32m      3\u001b[39m     data=bbc_text,\n\u001b[32m      4\u001b[39m     y=\u001b[33m\"\u001b[39m\u001b[33mcategory\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m     legend=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m      8\u001b[39m )\n\u001b[32m      9\u001b[39m plt.title(\u001b[33m\"\u001b[39m\u001b[33mNumber of Articles per Category\u001b[39m\u001b[33m\"\u001b[39m, fontsize=\u001b[32m14\u001b[39m, fontweight=\u001b[33m'\u001b[39m\u001b[33mbold\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(\n",
    "    data=bbc_text,\n",
    "    y=\"category\",\n",
    "    hue=\"category\",\n",
    "    palette=\"Set1\",\n",
    "    legend=False\n",
    ")\n",
    "plt.title(\"Number of Articles per Category\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Category\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nArticles per Category:\")\n",
    "print(bbc_text['category'].value_counts().sort_values(ascending=False))\n",
    "print(f\"\\nTotal articles: {len(bbc_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227ad0b3",
   "metadata": {},
   "source": [
    "## Stop Words Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbf9214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "\n",
    "# Download stopwords corpus\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "def plot_stopwords(data):\n",
    "    \"\"\"Visualize the top 10 most common stop words in the dataset\"\"\"\n",
    "    stop = set(stopwords.words('english'))\n",
    "    data_split = data.str.split()\n",
    "    data_list = data_split.values.tolist()\n",
    "    corpus = [word for i in data_list for word in i]\n",
    "    \n",
    "    dictionary_stopwords = defaultdict(int)\n",
    "    for word in corpus:\n",
    "        if word in stop:\n",
    "            dictionary_stopwords[word] += 1\n",
    "    \n",
    "    # Get top 10 stop words\n",
    "    top = sorted(dictionary_stopwords.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    x, y = zip(*top)\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x, y, color='steelblue')\n",
    "    plt.title('Top 10 Most Common Stop Words in BBC News Dataset', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Stop Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"✓ Stop words analysis completed\")\n",
    "    print(f\"\\nTop 10 Stop Words:\")\n",
    "    for word, count in top:\n",
    "        print(f\"  {word}: {count}\")\n",
    "\n",
    "# Run the analysis\n",
    "plot_stopwords(bbc_text['News_Headline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4ca79b",
   "metadata": {},
   "source": [
    "## Top Frequent Words Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3155e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def top_frequent_words(data):\n",
    "    \"\"\"Visualize the top 20 most frequent non-stopwords using a rainbow palette\"\"\"\n",
    "    stop = set(stopwords.words('english'))\n",
    "\n",
    "    # Convert to lowercase and split into words\n",
    "    data_split = data.str.lower().str.split()\n",
    "    data_list = data_split.values.tolist()\n",
    "    corpus = [word for i in data_list for word in i]\n",
    "\n",
    "    # Count word frequencies\n",
    "    counter = Counter(corpus)\n",
    "    most_common = counter.most_common()\n",
    "\n",
    "    # Extract top 20 non-stopwords (alphabetic only)\n",
    "    words, counts = [], []\n",
    "    for word, count in most_common:\n",
    "        if word not in stop and word.isalpha():\n",
    "            words.append(word)\n",
    "            counts.append(count)\n",
    "        if len(words) == 20:\n",
    "            break\n",
    "\n",
    "    # Create DataFrame for seaborn\n",
    "    plot_df = pd.DataFrame({\n",
    "        \"word\": words,\n",
    "        \"count\": counts\n",
    "    })\n",
    "\n",
    "    # Create visualization with rainbow palette\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(\n",
    "        data=plot_df,\n",
    "        x=\"count\",\n",
    "        y=\"word\",\n",
    "        hue=\"word\",\n",
    "        palette=\"rainbow\",\n",
    "        legend=False\n",
    "    )\n",
    "    plt.title(\"Top 20 Most Frequent Non-Stopwords in BBC News\", fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.ylabel(\"Word\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Top frequent words analysis completed\")\n",
    "    print(f\"\\nTop 20 Most Frequent Words:\")\n",
    "    for idx, (word, count) in enumerate(zip(words, counts), 1):\n",
    "        print(f\"  {idx:2d}. {word:15s}: {count:4d}\")\n",
    "\n",
    "# Run the analysis\n",
    "top_frequent_words(bbc_text[\"News_Headline\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff78fcc",
   "metadata": {},
   "source": [
    "## Word Cloud Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c5c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# Setup NLTK data directory\n",
    "NLTK_DATA_DIR = os.path.join(os.getcwd(), \"nltk_data\")\n",
    "os.makedirs(NLTK_DATA_DIR, exist_ok=True)\n",
    "\n",
    "if NLTK_DATA_DIR not in nltk.data.path:\n",
    "    nltk.data.path.insert(0, NLTK_DATA_DIR)\n",
    "\n",
    "# Download required NLTK packages\n",
    "for pkg in [\"stopwords\", \"wordnet\", \"punkt\", \"punkt_tab\", \"omw-1.4\"]:\n",
    "    try:\n",
    "        nltk.download(pkg, download_dir=NLTK_DATA_DIR, quiet=True)\n",
    "    except Exception as e:\n",
    "        print(f\"NLTK download warning for {pkg}: {e}\")\n",
    "\n",
    "# Initialize tokenizer and lemmatizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def plot_wordcloud(data):\n",
    "    \"\"\"Generate and display a word cloud from the dataset\"\"\"\n",
    "    stop = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    corpus = []\n",
    "    for text in data:\n",
    "        # Tokenize and normalize\n",
    "        tokens = tokenizer.tokenize(str(text).lower())\n",
    "        # Filter: alphabetic, non-stopwords, length > 2, and lemmatize\n",
    "        tokens = [\n",
    "            lemmatizer.lemmatize(w)\n",
    "            for w in tokens\n",
    "            if w.isalpha() and w not in stop and len(w) > 2\n",
    "        ]\n",
    "        corpus.extend(tokens)\n",
    "\n",
    "    # Generate word cloud\n",
    "    wc = WordCloud(\n",
    "        background_color=\"white\",\n",
    "        stopwords=STOPWORDS,\n",
    "        max_words=200,\n",
    "        max_font_size=40,\n",
    "        scale=3,\n",
    "        random_state=1,\n",
    "        colormap='viridis'\n",
    "    ).generate(\" \".join(corpus))\n",
    "\n",
    "    # Display\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.title(\"Word Cloud - BBC News Articles\", fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Word cloud visualization completed!\")\n",
    "    print(f\"Total unique words in cloud: {len(wc.words_)}\")\n",
    "\n",
    "# Generate the word cloud\n",
    "plot_wordcloud(bbc_text[\"News_Headline\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76db50fb",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Cleaning\n",
    "\n",
    "Before building our model, we need to clean and preprocess the dataset to remove stop words, punctuation, inconsistencies, and other noise that could cause the model to under/overfit. We'll apply several techniques: lowercasing, tokenization, lemmatization, and stop word removal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5968e2",
   "metadata": {},
   "source": [
    "### Step 1: Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604fba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all text to lowercase for consistency\n",
    "print(\"Converting text to lowercase...\")\n",
    "bbc_text_clean = bbc_text.copy()\n",
    "bbc_text_clean['News_Headline'] = bbc_text_clean['News_Headline'].apply(lambda x: str(x).lower())\n",
    "\n",
    "print(\"✓ Lowercasing completed\")\n",
    "print(f\"\\nSample before and after:\")\n",
    "print(f\"Original: {bbc_text['News_Headline'].iloc[0][:100]}\")\n",
    "print(f\"Lowercase: {bbc_text_clean['News_Headline'].iloc[0][:100]}\")\n",
    "print(f\"\\nDataset shape: {bbc_text_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e39542",
   "metadata": {},
   "source": [
    "### Step 2: Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08449446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words from the text\n",
    "print(\"Removing stop words...\")\n",
    "\n",
    "# Get English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize and remove stop words\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Tokenize text and remove stop words\"\"\"\n",
    "    tokens = str(text).split()\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Apply to the dataset\n",
    "bbc_text_clean['News_Headline'] = bbc_text_clean['News_Headline'].apply(remove_stopwords)\n",
    "\n",
    "print(\"✓ Stop words removal completed\")\n",
    "print(f\"\\nNumber of stop words removed: {len(stop_words)}\")\n",
    "print(f\"Sample stop words: {list(stop_words)[:15]}\")\n",
    "\n",
    "print(f\"\\nSample after stop word removal:\")\n",
    "print(f\"Text: {bbc_text_clean['News_Headline'].iloc[0][:100]}\")\n",
    "\n",
    "print(f\"\\nDataset shape: {bbc_text_clean.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(bbc_text_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92765e83",
   "metadata": {},
   "source": [
    "### Step 3: Stemming and Lemmatization\n",
    "\n",
    "**Stemming** removes the last few characters from a word to get its root form, but can produce non-words (e.g., \"running\" → \"runn\").\n",
    "\n",
    "**Lemmatization** reduces words to their dictionary base form (lemma) using context, producing valid words (e.g., \"running\" → \"run\", \"better\" → \"good\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc240340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Download wordnet if not already available\n",
    "nltk.download('wordnet', download_dir=NLTK_DATA_DIR, quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir=NLTK_DATA_DIR, quiet=True)\n",
    "\n",
    "# Demonstration of Stemming vs Lemmatization\n",
    "sample_words = ['running', 'runs', 'ran', 'runner', 'walked', 'walks', 'better', 'study', 'studying']\n",
    "print(\"=\" * 60)\n",
    "print(\"STEMMING vs LEMMATIZATION COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Word':<15} {'Stemmed':<15} {'Lemmatized':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for word in sample_words:\n",
    "    stemmed = stemmer.stem(word)\n",
    "    lemmatized = lemmatizer.lemmatize(word, pos='v')  # pos='v' for verbs\n",
    "    print(f\"{word:<15} {stemmed:<15} {lemmatized:<15}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Applying Lemmatization to dataset...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Function to lemmatize text\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"Tokenize text and apply lemmatization\"\"\"\n",
    "    tokens = str(text).split()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Apply lemmatization to the dataset\n",
    "bbc_text_clean['News_Headline'] = bbc_text_clean['News_Headline'].apply(lemmatize_text)\n",
    "\n",
    "print(\"✓ Lemmatization completed\")\n",
    "print(f\"\\nSample after lemmatization:\")\n",
    "print(f\"Text: {bbc_text_clean['News_Headline'].iloc[0][:100]}\")\n",
    "print(f\"\\nDataset shape: {bbc_text_clean.shape}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "print(bbc_text_clean.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aa43e6",
   "metadata": {},
   "source": [
    "## 3. Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a2aabcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1335\n",
      "Testing set size:  890\n",
      "Class distribution (train):\n",
      "category\n",
      "sport            307\n",
      "business         306\n",
      "politics         250\n",
      "tech             240\n",
      "entertainment    232\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution (test):\n",
      "category\n",
      "sport            204\n",
      "business         204\n",
      "politics         167\n",
      "tech             161\n",
      "entertainment    154\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare feature and target\n",
    "if 'text_clean' in bbc_text.columns:\n",
    "    X = bbc_text['text_clean']\n",
    "elif 'text_clean_str' in bbc_text.columns:\n",
    "    X = bbc_text['text_clean_str']\n",
    "else:\n",
    "    # fallback to raw headlines\n",
    "    X = bbc_text['News_Headline'].astype(str)\n",
    "\n",
    "y = bbc_text['category']\n",
    "\n",
    "# Split data with 60% train, 40% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.6, random_state=1, stratify=y if 'category' in bbc_text.columns else None\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size:  {len(X_test)}\")\n",
    "print(f\"Class distribution (train):\\n{y_train.value_counts()}\\n\")\n",
    "print(f\"Class distribution (test):\\n{y_test.value_counts()}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c873dfd",
   "metadata": {},
   "source": [
    "## 4. Feature Transformation (TF-IDF Vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1857b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF (may take a moment)...\n",
      "✓ TF-IDF completed — shape: (2225, 1000) (documents, features)\n"
     ]
    }
   ],
   "source": [
    "# Robust TF-IDF encoding step: ensure data is available and use cleaned text if present\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "\n",
    "# Attempt to load CSV; if missing, rebuild from folder\n",
    "try:\n",
    "    if 'bbc_text' not in globals():\n",
    "        bbc_text = pd.read_csv('/workspaces/BBCnews/bbc-text.csv')\n",
    "        if 'News_Headline' not in bbc_text.columns and 'text' in bbc_text.columns:\n",
    "            bbc_text = bbc_text.rename(columns={'text': 'News_Headline'}, inplace=False)\n",
    "except Exception:\n",
    "    print('bbc-text.csv not found; rebuilding dataset from folder...')\n",
    "    texts = []\n",
    "    categories = []\n",
    "    base_path = '/workspaces/BBCnews/bbc'\n",
    "    for category in os.listdir(base_path):\n",
    "        category_path = os.path.join(base_path, category)\n",
    "        if not os.path.isdir(category_path):\n",
    "            continue\n",
    "        for filename in sorted(os.listdir(category_path)):\n",
    "            file_path = os.path.join(category_path, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='latin-1') as f:\n",
    "                    texts.append(f.read())\n",
    "                    categories.append(category)\n",
    "            except Exception as e:\n",
    "                continue\n",
    "    bbc_text = pd.DataFrame({'News_Headline': texts, 'category': categories})\n",
    "\n",
    "# Prefer cleaned dataframe if available\n",
    "if 'bbc_text_clean' in globals():\n",
    "    source_series = bbc_text_clean['News_Headline'].astype(str)\n",
    "else:\n",
    "    # fall back to original\n",
    "    source_series = bbc_text['News_Headline'].astype(str)\n",
    "\n",
    "# Create a cleaned string column if not present\n",
    "if 'text_clean_str' not in bbc_text.columns:\n",
    "    # If token lists exist in 'text_clean', join them; otherwise use source_series lowercase\n",
    "    if 'text_clean' in bbc_text.columns:\n",
    "        bbc_text['text_clean_str'] = bbc_text['text_clean'].apply(lambda x: ' '.join(x) if isinstance(x, (list, tuple)) else str(x))\n",
    "    else:\n",
    "        bbc_text['text_clean_str'] = source_series.str.lower()\n",
    "\n",
    "# TF-IDF vectorization\n",
    "print('Fitting TF-IDF (may take a moment)...')\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=1000,\n",
    "    lowercase=True,\n",
    "    analyzer='word',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1,1)\n",
    ")\n",
    "\n",
    "tf_idf1 = vectorizer.fit_transform(bbc_text['text_clean_str'])\n",
    "print(f'✓ TF-IDF completed — shape: {tf_idf1.shape} (documents, features)')\n",
    "\n",
    "# Keep vectorizer for reuse\n",
    "tfidf_vectorizer = vectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a052b164",
   "metadata": {},
   "source": [
    "## 5. Train Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e203c904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and train the Logistic Regression model\n",
    "logistic_reg = LogisticRegression(max_iter=1000, random_state=42, multi_class='multinomial')\n",
    "\n",
    "print(\"Training Logistic Regression model...\")\n",
    "logistic_reg.fit(X_train_transformed, y_train)\n",
    "\n",
    "print(\"✓ Model training completed!\")\n",
    "print(f\"Model: {logistic_reg}\")\n",
    "print(f\"Number of classes: {len(logistic_reg.classes_)}\")\n",
    "print(f\"Classes: {logistic_reg.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f747a7b5",
   "metadata": {},
   "source": [
    "## 6. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a67a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on training and test sets\n",
    "y_pred_train = logistic_reg.predict(X_train_transformed)\n",
    "y_pred_test = logistic_reg.predict(X_test_transformed)\n",
    "\n",
    "# Get prediction probabilities\n",
    "y_pred_proba = logistic_reg.predict_proba(X_test_transformed)\n",
    "\n",
    "print(f\"✓ Predictions generated!\")\n",
    "print(f\"Sample predictions (first 10): {y_pred_test[:10].tolist()}\")\n",
    "print(f\"Confidence scores for first sample: {dict(zip(logistic_reg.classes_, y_pred_proba[0]))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ad548",
   "metadata": {},
   "source": [
    "## 7. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c7c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "precision = precision_score(y_test, y_pred_test, average='weighted')\n",
    "recall = recall_score(y_test, y_pred_test, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred_test, average='weighted')\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Testing Accuracy:  {test_accuracy:.4f}\")\n",
    "print(f\"Precision (weighted): {precision:.4f}\")\n",
    "print(f\"Recall (weighted):    {recall:.4f}\")\n",
    "print(f\"F1-Score (weighted):  {f1:.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de209cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "print(\"\\nDETAILED CLASSIFICATION REPORT:\")\n",
    "print(\"=\" * 50)\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "979cbe1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Confusion Matrix\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m cm = \u001b[43mconfusion_matrix\u001b[49m(y_test, y_pred_test)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Visualize confusion matrix\u001b[39;00m\n\u001b[32m      5\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m8\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name 'confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=logistic_reg.classes_, \n",
    "            yticklabels=logistic_reg.classes_)\n",
    "plt.title('Confusion Matrix - Logistic Regression', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Confusion matrix visualization completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccee901e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1266.66s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# In a notebook cell:\n",
    "%pip install -q pandas scikit-learn nltk wordcloud seaborn matplotlib\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
